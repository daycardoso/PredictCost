{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPn+lUj+LUeV1aMSDsuouqp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daycardoso/PredictCost/blob/main/PredicCostRandomForestRegressor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VE2h1wBG4J_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea17894-9f87-4419-c60f-8b6e56ab9490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add `%load_ext cuml.accel` before importing sklearn to speed up operations using GPU\n"
      ],
      "metadata": {
        "id": "pw_pWH6HS_wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cuml\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import cudf\n",
        "# from cuml.ensemble import RandomForestRegressor\n",
        "# # Correct the import for RandomizedSearchCV\n",
        "# from sklearn.model_selection import RandomizedSearchCV\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import (\n",
        "#     r2_score, mean_squared_error, mean_absolute_error,\n",
        "#     mean_absolute_percentage_error, median_absolute_error,\n",
        "#     max_error, explained_variance_score\n",
        "# )\n",
        "# import matplotlib.pyplot as plt\n",
        "# import joblib\n",
        "# import json\n",
        "\n",
        "# # 1. Carregamento do dataset\n",
        "# df_unificado = pd.read_csv('/content/drive/MyDrive/Trabalho ML Mestrado 01-2025/df_unificado.csv')\n",
        "# X = df_unificado.iloc[:, :-1]\n",
        "# y = df_unificado.iloc[:, -1]\n",
        "# feature_names = df_unificado.columns[:-1]\n",
        "# target_name = df_unificado.columns[-1]\n",
        "# print(f\"Dimensões de X: {X.shape}\")\n",
        "# print(f\"Dimensões de y: {y.shape}\")\n",
        "# print(f\"Nomes dos atributos: {feature_names.tolist()}\")\n",
        "# print(f\"Nome da variável target: {target_name}\")\n",
        "# print(f\"Tamanho do dataset: {len(df_unificado)}\")\n",
        "\n",
        "# # 2. Conversão para cudf\n",
        "# X_cudf = cudf.DataFrame.from_pandas(X)\n",
        "# y_cudf = cudf.Series(y)\n",
        "\n",
        "# # 3. Split Hold-out para avaliação final (antes do tuning)\n",
        "# X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "#     X_cudf, y_cudf, test_size=0.3, random_state=42\n",
        "# )\n",
        "\n",
        "# # 4. Amostragem para tuning (~100k exemplos)\n",
        "# amostra_pct = min(100_000 / len(X_train_full), 1.0)\n",
        "# X_tune, _, y_tune, _ = train_test_split(\n",
        "#     X_train_full, y_train_full, train_size=amostra_pct, random_state=42\n",
        "# )\n",
        "# print(f\"Amostra para tuning: {len(X_tune)} exemplos.\")\n",
        "\n",
        "# # 5. Espaço de busca RandomizedSearchCV\n",
        "# from scipy.stats import randint\n",
        "# param_dist = {\n",
        "#     'n_estimators': randint(50, 250),\n",
        "#     'max_depth': randint(3, 25),\n",
        "#     'max_features': ['sqrt', 'log2', 0.2, 0.5],\n",
        "#     'min_samples_leaf': randint(1, 5),\n",
        "#     'bootstrap': [True, False]\n",
        "# }\n",
        "\n",
        "# rf = RandomForestRegressor(random_state=42)\n",
        "# search = RandomizedSearchCV(\n",
        "#     rf,\n",
        "#     param_distributions=param_dist,\n",
        "#     n_iter=20,\n",
        "#     scoring='neg_mean_squared_error',\n",
        "#     cv=3,\n",
        "#     random_state=42,\n",
        "#     verbose=2\n",
        "# )\n",
        "\n",
        "# # 6. Tuning dos hiperparâmetros\n",
        "# print(\"\\nIniciando tuning...\")\n",
        "# search.fit(X_tune, y_tune)\n",
        "# print(\"Melhores parâmetros encontrados:\", search.best_params_)\n",
        "\n",
        "# # Early stopping \"manual\":\n",
        "# scores = -search.cv_results_['mean_test_score']\n",
        "# improvements = np.diff(scores)\n",
        "# if len(improvements) > 0 and np.abs(improvements).max() < 0.01 * np.mean(scores):\n",
        "#     print(\"Melhorias marginais. Early stopping aplicado.\")\n",
        "\n",
        "# # 7. Treinamento do modelo final com todos os dados de treino\n",
        "# print(\"\\nTreinando modelo final com todos os dados disponíveis...\")\n",
        "# best_params = search.best_params_\n",
        "# rf_final = RandomForestRegressor(**best_params, random_state=42)\n",
        "# rf_final.fit(X_train_full, y_train_full)\n",
        "\n",
        "# # 8. Avaliação no hold-out\n",
        "# y_pred_gpu = rf_final.predict(X_test)\n",
        "# y_pred = y_pred_gpu.to_numpy()\n",
        "# y_test_cpu = y_test.to_numpy()\n",
        "\n",
        "# metrics = {\n",
        "#     'R² Score'          : r2_score(y_test_cpu, y_pred),\n",
        "#     'MSE'               : mean_squared_error(y_test_cpu, y_pred),\n",
        "#     'MAE'               : mean_absolute_error(y_test_cpu, y_pred),\n",
        "#     'MAPE'              : mean_absolute_percentage_error(y_test_cpu, y_pred),\n",
        "#     'Median AE'         : median_absolute_error(y_test_cpu, y_pred),\n",
        "#     'Max Error'         : max_error(y_test_cpu, y_pred),\n",
        "#     'Explained Variance': explained_variance_score(y_test_cpu, y_pred)\n",
        "# }\n",
        "# df_metrics = pd.DataFrame.from_dict(metrics, orient='index', columns=['Valor']).round(4)\n",
        "\n",
        "# # 9. Relatório visual\n",
        "# plt.figure()\n",
        "# plt.scatter(y_test_cpu, y_pred, alpha=0.5)\n",
        "# min_val, max_val = np.min([y_test_cpu.min(), y_pred.min()]), np.max([y_test_cpu.max(), y_pred.max()])\n",
        "# plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
        "# plt.xlabel('Valor Real')\n",
        "# plt.ylabel('Valor Predito')\n",
        "# plt.title('Real vs Predito')\n",
        "# plt.grid(True)\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# residuals = y_test_cpu - y_pred\n",
        "# plt.figure()\n",
        "# plt.hist(residuals, bins=50)\n",
        "# plt.xlabel('Resíduo (Real - Predito)')\n",
        "# plt.title('Histograma de Resíduos')\n",
        "# plt.grid(True)\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# df_r = pd.DataFrame({'Real': y_test_cpu, 'Resíduo': residuals})\n",
        "# df_r['Quartil'] = pd.qcut(df_r['Real'], 4, labels=[1, 2, 3, 4])\n",
        "# groups = [df_r[df_r['Quartil'] == q]['Resíduo'] for q in sorted(df_r['Quartil'].unique())]\n",
        "# plt.figure()\n",
        "# plt.boxplot(groups, labels=sorted(df_r['Quartil'].unique()))\n",
        "# plt.xlabel('Quartil de Valor Real')\n",
        "# plt.ylabel('Resíduo')\n",
        "# plt.title('Boxplot de Resíduos por Quartil')\n",
        "# plt.grid(True)\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# if hasattr(rf_final, 'feature_importances_') and feature_names is not None:\n",
        "#     fi = pd.Series(rf_final.feature_importances_.to_numpy(), index=feature_names).sort_values(ascending=False)\n",
        "#     plt.figure(figsize=(10, 4))\n",
        "#     fi.plot(kind='bar')\n",
        "#     plt.ylabel('Importância')\n",
        "#     plt.title('Importância das Features')\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# # 10. Salvar resultados e parâmetros\n",
        "# full_results = {\n",
        "#     'pipeline': rf_final,\n",
        "#     'X_test':   X_test.to_pandas(),\n",
        "#     'y_test':   y_test_cpu,\n",
        "#     'y_pred':   y_pred,\n",
        "#     'metrics':  df_metrics,\n",
        "#     'best_params': best_params,\n",
        "#     'feature_names': X.columns.tolist()\n",
        "# }\n",
        "# joblib.dump(full_results, '/content/drive/MyDrive/Trabalho ML Mestrado 01-2025/rf_regressor_final_full_results.pkl')\n",
        "\n",
        "# print(\"\\nRelatório Final:\")\n",
        "# print(\"Melhores hiperparâmetros:\", best_params)\n",
        "# print(\"\\nMétricas no Hold-out:\")\n",
        "# print(df_metrics)\n",
        "\n",
        "# # Top 10 features\n",
        "# feature_importances = None\n",
        "# if hasattr(rf_final, 'feature_importances_') and feature_names is not None:\n",
        "#     fi = pd.Series(rf_final.feature_importances_.to_numpy(), index=feature_names).sort_values(ascending=False)\n",
        "#     top_features = fi.head(10).to_dict()\n",
        "#     feature_importances = fi.to_dict()\n",
        "# else:\n",
        "#     top_features = None\n",
        "\n",
        "# # Relatório JSON\n",
        "# relatorio = {\n",
        "#     'dataset': {\n",
        "#         'total_samples': int(len(df_unificado)),\n",
        "#         'n_features': int(X.shape[1]),\n",
        "#         'feature_names': feature_names.tolist(),\n",
        "#         'target_name': target_name\n",
        "#     },\n",
        "#     'tuning': {\n",
        "#         'sample_size': int(len(X_tune)),\n",
        "#         'param_dist': {k: (str(v) if not isinstance(v, (int, float, list, tuple, dict)) else v) for k, v in param_dist.items()},\n",
        "#         'n_iter': int(search.n_iter),\n",
        "#         'cv': int(search.cv),\n",
        "#         'random_state': int(search.random_state),\n",
        "#     },\n",
        "#     'best_params': best_params,\n",
        "#     'metrics': df_metrics['Valor'].to_dict(),\n",
        "#     'feature_importances_top10': top_features,\n",
        "#     'full_feature_importances': feature_importances if feature_importances is not None else None,\n",
        "#     'predictions': {\n",
        "#         'real': y_test_cpu.tolist()[:1000],     # Salve apenas os 1000 primeiros para não ficar gigante\n",
        "#         'predicted': y_pred.tolist()[:1000],\n",
        "#         'residuals': residuals.tolist()[:1000]\n",
        "#     },\n",
        "#     'artifacts': {\n",
        "#         'model_pkl': '/content/drive/MyDrive/Trabalho ML Mestrado 01-2025/rf_regressor_final_full_results.pkl'\n",
        "#     }\n",
        "# }\n",
        "\n",
        "# # Salvando relatório JSON\n",
        "# relatorio_path = '/content/drive/MyDrive/Trabalho ML Mestrado 01-2025/relatorio_rf_final.json'\n",
        "# with open(relatorio_path, 'w') as f:\n",
        "#     json.dump(relatorio, f, indent=4)\n",
        "\n",
        "# print(f\"Relatório salvo em: {relatorio_path}\")\n"
      ],
      "metadata": {
        "id": "HHVtgaSjfEUT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# 0. Instalação XGBoost (Colab geralmente já tem, mas garante versão atual)\n",
        "# ======================\n",
        "!pip install --upgrade xgboost scikit-learn\n",
        "\n",
        "# ======================\n",
        "# 1. Imports\n",
        "# ======================\n",
        "import pandas as pd\n",
        "import cudf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import json\n",
        "from cuml.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import (\n",
        "    r2_score, mean_squared_error, mean_absolute_error,\n",
        "    mean_absolute_percentage_error, median_absolute_error,\n",
        "    max_error, explained_variance_score\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# 2. Dados\n",
        "# ======================\n",
        "df_unificado = pd.read_csv('/content/drive/MyDrive/Trabalho ML Mestrado 01-2025/df_unificado.csv')\n",
        "X = df_unificado.iloc[:, :-1]\n",
        "y = df_unificado.iloc[:, -1]\n",
        "feature_names = df_unificado.columns[:-1]\n",
        "\n",
        "X_cudf = cudf.DataFrame.from_pandas(X)\n",
        "y_cudf = cudf.Series(y)\n",
        "\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    X_cudf, y_cudf, test_size=0.3, random_state=42\n",
        ")\n",
        "X_train_np = X_train_full.to_numpy()\n",
        "y_train_np = y_train_full.to_numpy()\n",
        "X_test_np  = X_test.to_numpy()\n",
        "y_test_np  = y_test.to_numpy()\n",
        "\n",
        "def avalia(y_true, y_pred):\n",
        "    return {\n",
        "        'R² Score'          : float(r2_score(y_true, y_pred)),\n",
        "        'MSE'               : float(mean_squared_error(y_true, y_pred)),\n",
        "        'MAE'               : float(mean_absolute_error(y_true, y_pred)),\n",
        "        'MAPE'              : float(mean_absolute_percentage_error(y_true, y_pred)),\n",
        "        'Median AE'         : float(median_absolute_error(y_true, y_pred)),\n",
        "        'Max Error'         : float(max_error(y_true, y_pred)),\n",
        "        'Explained Variance': float(explained_variance_score(y_true, y_pred))\n",
        "    }\n",
        "\n",
        "# ======================\n",
        "# 3. Tuning Inteligente - Random Forest (cuML)\n",
        "# ======================\n",
        "print(\"\\n[RF] Tuning inteligente usando amostragem...\")\n",
        "\n",
        "# Amostra para tuning\n",
        "amostra_pct = min(100_000 / len(X_train_full), 1.0)\n",
        "X_tune, _, y_tune, _ = train_test_split(\n",
        "    X_train_full, y_train_full, train_size=amostra_pct, random_state=42\n",
        ")\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [80, 120, 200],\n",
        "    'max_depth': [10, 16, 24],\n",
        "    'max_features': ['sqrt', 0.7, 1.0],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "best_score = -np.inf\n",
        "best_params_rf = None\n",
        "\n",
        "for n in param_grid['n_estimators']:\n",
        "    for d in param_grid['max_depth']:\n",
        "        for mf in param_grid['max_features']:\n",
        "            for leaf in param_grid['min_samples_leaf']:\n",
        "                try:\n",
        "                    rf_t = RandomForestRegressor(\n",
        "                        n_estimators=n,\n",
        "                        max_depth=d,\n",
        "                        max_features=mf,\n",
        "                        min_samples_leaf=leaf,\n",
        "                        n_streams=1,\n",
        "                        random_state=42\n",
        "                    )\n",
        "                    rf_t.fit(X_tune, y_tune)\n",
        "                    y_pred = rf_t.predict(X_tune).to_numpy()\n",
        "                    score = r2_score(y_tune.to_numpy(), y_pred)\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_params_rf = {'n_estimators': n, 'max_depth': d,\n",
        "                                          'max_features': mf, 'min_samples_leaf': leaf}\n",
        "                except Exception as e:\n",
        "                    print(f\"[RF] Erro com {n}, {d}, {mf}, {leaf}: {str(e)}\")\n",
        "print(f\"Melhores hiperparâmetros RF: {best_params_rf}, R2 (amostra): {best_score:.4f}\")\n",
        "\n",
        "# Treina RF final no conjunto completo\n",
        "rf_final = RandomForestRegressor(**best_params_rf, random_state=42)\n",
        "rf_final.fit(X_train_full, y_train_full)\n",
        "y_pred_rf = rf_final.predict(X_test).to_numpy()\n",
        "metrics_rf = avalia(y_test_np, y_pred_rf)\n",
        "try:\n",
        "    fi_rf = pd.Series(rf_final.feature_importances_.to_numpy(), index=feature_names).sort_values(ascending=False)\n",
        "except Exception as e:\n",
        "    print(\"Feature importance indisponível no cuML RandomForest:\", str(e))\n",
        "    fi_rf = None\n",
        "\n",
        "# ======================\n",
        "# 4. Tuning Inteligente - XGBoost (GPU) com RandomizedSearchCV\n",
        "# ======================\n",
        "print(\"\\n[XGBoost] Tuning inteligente usando amostragem e RandomizedSearchCV...\")\n",
        "\n",
        "# Amostra para tuning\n",
        "X_tune_np, _, y_tune_np, _ = train_test_split(\n",
        "    X_train_np, y_train_np, train_size=amostra_pct, random_state=42\n",
        ")\n",
        "\n",
        "xgb_param_dist = {\n",
        "    'n_estimators':      [80, 120, 200],\n",
        "    'max_depth':         [6, 10, 16],\n",
        "    'learning_rate':     [0.05, 0.1, 0.2],\n",
        "    'subsample':         [0.7, 0.9, 1.0],\n",
        "    'colsample_bytree':  [0.7, 1.0],\n",
        "    'gamma':             [0, 1],\n",
        "    'reg_alpha':         [0, 1],\n",
        "    'reg_lambda':        [1, 2]\n",
        "}\n",
        "xgb_reg = xgb.XGBRegressor(\n",
        "    tree_method=\"gpu_hist\",\n",
        "    random_state=42,\n",
        "    verbosity=1\n",
        ")\n",
        "\n",
        "xgb_search = RandomizedSearchCV(\n",
        "    xgb_reg,\n",
        "    param_distributions=xgb_param_dist,\n",
        "    n_iter=10,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=3,\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    n_jobs=1\n",
        ")\n",
        "\n",
        "xgb_search.fit(X_tune_np, y_tune_np)\n",
        "best_xgb_params = xgb_search.best_params_\n",
        "print(\"Melhores hiperparâmetros XGBoost:\", best_xgb_params)\n",
        "\n",
        "# Treinamento final XGBoost com os melhores hiperparâmetros\n",
        "xgb_final = xgb.XGBRegressor(**best_xgb_params, tree_method=\"gpu_hist\", random_state=42)\n",
        "xgb_final.fit(X_train_np, y_train_np)\n",
        "y_pred_xgb = xgb_final.predict(X_test_np)\n",
        "metrics_xgb = avalia(y_test_np, y_pred_xgb)\n",
        "fi_xgb = pd.Series(xgb_final.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
        "\n",
        "# ======================\n",
        "# 5. Visualizações comparativas\n",
        "# ======================\n",
        "res_rf  = y_test_np - y_pred_rf\n",
        "res_xgb = y_test_np - y_pred_xgb\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "for i, (ypred, name) in enumerate(zip([y_pred_rf, y_pred_xgb], ['Random Forest', 'XGBoost'])):\n",
        "    plt.subplot(1, 2, i+1)\n",
        "    plt.scatter(y_test_np[:2000], ypred[:2000], alpha=0.4, s=5)\n",
        "    min_val, max_val = np.min([y_test_np.min(), ypred.min()]), np.max([y_test_np.max(), ypred.max()])\n",
        "    plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
        "    plt.xlabel('Valor Real')\n",
        "    plt.ylabel('Predito')\n",
        "    plt.title(f'{name}: Real vs Predito')\n",
        "    plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "for i, (res, name) in enumerate(zip([res_rf, res_xgb], ['Random Forest', 'XGBoost'])):\n",
        "    plt.subplot(1, 2, i+1)\n",
        "    plt.hist(res, bins=50, alpha=0.8)\n",
        "    plt.xlabel('Resíduo (Real - Predito)')\n",
        "    plt.title(f'{name} - Histograma de Resíduos')\n",
        "    plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "df_plot = pd.DataFrame({\n",
        "    'Real': y_test_np,\n",
        "    'Resíduo_RF': res_rf,\n",
        "    'Resíduo_XGB': res_xgb\n",
        "})\n",
        "df_plot['Quartil'] = pd.qcut(df_plot['Real'], 4, labels=[1, 2, 3, 4])\n",
        "plt.figure(figsize=(12, 4))\n",
        "for i, (res_col, name) in enumerate(zip(['Resíduo_RF', 'Resíduo_XGB'],\n",
        "                                        ['Random Forest', 'XGBoost'])):\n",
        "    groups = [df_plot[df_plot['Quartil'] == q][res_col] for q in sorted(df_plot['Quartil'].unique())]\n",
        "    plt.subplot(1, 2, i+1)\n",
        "    plt.boxplot(groups, labels=sorted(df_plot['Quartil'].unique()))\n",
        "    plt.xlabel('Quartil do Valor Real')\n",
        "    plt.ylabel('Resíduo')\n",
        "    plt.title(f'{name}: Resíduos por Quartil')\n",
        "    plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "for i, (fi, name) in enumerate(zip([fi_rf, fi_xgb], ['RF', 'XGBoost'])):\n",
        "    plt.subplot(1, 2, i+1)\n",
        "    if fi is not None:\n",
        "        fi.head(10).plot(kind='bar')\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'N/A', fontsize=14, ha='center')\n",
        "    plt.title(f'{name} - Top 10 Importância')\n",
        "    plt.ylabel('Importância')\n",
        "    plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ======================\n",
        "# 6. Salvar relatórios e modelos\n",
        "# ======================\n",
        "resultados = {\n",
        "    'metrics': {\n",
        "        'RandomForest': metrics_rf,\n",
        "        'XGBoost_GPU': metrics_xgb\n",
        "    },\n",
        "    'best_params': {\n",
        "        'RandomForest': best_params_rf,\n",
        "        'XGBoost_GPU': best_xgb_params\n",
        "    },\n",
        "    'feature_importances': {\n",
        "        'RandomForest': fi_rf.to_dict() if fi_rf is not None else None,\n",
        "        'XGBoost_GPU': fi_xgb.to_dict()\n",
        "    }\n",
        "}\n",
        "with open('/content/drive/MyDrive/Trabalho ML Mestrado 01-2025/relatorio_comparativo_rf_xgb_tuning.json', 'w') as f:\n",
        "    json.dump(resultados, f, indent=4)\n",
        "\n",
        "joblib.dump(rf_final,   '/content/drive/MyDrive/Trabalho ML Mestrado 01-2025/rf_final_tuned.pkl')\n",
        "joblib.dump(xgb_final, '/content/drive/MyDrive/Trabalho ML Mestrado 01-2025/xgb_final_tuned.pkl')\n",
        "\n",
        "print('\\nPipeline tuning concluído! Confira métricas, gráficos e modelos!')\n"
      ],
      "metadata": {
        "id": "c8c5R0aPjxOk",
        "outputId": "42ea2c01-2a9e-4d22-b8ae-b5133a7f2dea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (3.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/cudf/core/dataframe.py:7737: FutureWarning: Using `__dataframe__` is deprecated\n",
            "  warnings.warn(\"Using `__dataframe__` is deprecated\", FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[RF] Tuning inteligente usando amostragem...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/cudf/core/dataframe.py:7737: FutureWarning: Using `__dataframe__` is deprecated\n",
            "  warnings.warn(\"Using `__dataframe__` is deprecated\", FutureWarning)\n"
          ]
        }
      ]
    }
  ]
}