{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daycardoso/PredictCost/blob/main/PredictCostRandonForest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trabalho CMP263 - Aprendizagem de M√°quina - INF/UFRGS\n",
        "\n",
        "## Modelo 1 - Decision tree sem poda\n",
        "\n",
        "As √°rvores de decis√£o s√£o conhecidas por possu√≠rem um baixo vi√©s, ao mesmo tempo em que apresentam alta vari√¢ncia.\n",
        "Isto √©, o m√©todo √© capaz de modelar fronteiras de decis√£o bastante complexas, o que, por um lado, √© positivo, mas torna o algoritmo bastante suscet√≠vel a ru√≠do ou a padr√µes nos dados de treino que n√£o generalizam para inst√¢ncias de teste.\n",
        "Por isso, t√©cnicas de poda s√£o fundamentais para o uso efetivo do modelo em dados novos.\n",
        "\n",
        "Nessa atividade, iremos analisar como a estrutura e as predi√ß√µes da √°rvore de decis√£o s√£o afetadas por pequenas varia√ß√µes no conjunto de treino. Al√©m disso, veremos duas t√©cnicas de poda que podem ser usadas para controlar a complexidade do modelo.\n",
        "\n",
        "**Este *colab* deve ser usado como base para o preenchimento do question√°rio encontrado no Moodle. Fa√ßa uma c√≥pia do mesmo para realizar o exerc√≠cio.** A forma mais f√°cil para duplicar este *colab* √© ir em File > \"Save a Copy in Drive\". N√£o √© necess√°rio entregar este *colab* preenchido, mas guarde-o para caso ache que algum question√°rio est√° errado.\n"
      ],
      "metadata": {
        "id": "E650rvooE65t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objetivos da Atividade\n",
        "* Analisar os impactos da caracter√≠stica de **vari√¢ncia** nas √°rvores de decis√£o.\n",
        "* Analisar o efeito da **poda** em √°rvores de decis√£o.\n"
      ],
      "metadata": {
        "id": "bzW8jdj7Jgp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregamento dos Dados\n"
      ],
      "metadata": {
        "id": "owi-J_whK4IS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Obten√ß√£o e an√°lise dos dados\n",
        "O c√≥digo abaixo carrega o dataset do kaggle e mostra algumas informa√ß√µes b√°sicas sobre os dados"
      ],
      "metadata": {
        "id": "SqJsx9OITVn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gqs1uPW95__c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# import glob\n",
        "\n",
        "# arquivos = glob.glob('/content/drive/MyDrive/Trabalho ML Mestrado 01-2025/*.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "rfTSbcxF_Ei6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dfs = [pd.read_csv(f) for f in arquivos]\n",
        "# df_unificado = pd.concat(dfs, axis=0, ignore_index=True)\n",
        "# df_unificado.head()"
      ],
      "metadata": {
        "id": "ACuMo2cjIP7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Excluir a coluna type\n",
        "# df_unificado = df_unificado.drop('type', axis=1)\n",
        "# df_unificado.head()"
      ],
      "metadata": {
        "id": "TyjiqnDQDhTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Garantir que n√£o a duplicata de instancias evitando sobreposi√ß√£o entre os dados de treinamento e teste\n",
        "# df_unificado = df_unificado.drop_duplicates().reset_index(drop=True)\n",
        "# df_unificado.head(-50)"
      ],
      "metadata": {
        "id": "w6WTOZinHWWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvar o dataset completo, sem duplicatas\n",
        "# df_unificado.to_csv('/content/drive/MyDrive/Trabalho ML Mestrado 01-2025/df_unificado.csv', index=False)"
      ],
      "metadata": {
        "id": "P5TiN0ACFnPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o datset unificado\n",
        "df_unificado = pd.read_csv('/content/drive/MyDrive/Trabalho ML Mestrado 01-2025/df_unificado.csv')"
      ],
      "metadata": {
        "id": "zGi6kt1-Hiai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# matriz contendo os atributos\n",
        "X = df_unificado.iloc[:, :-1].values\n",
        "\n",
        "# vetor contendo o custo, ou seja, a ultima coluna\n",
        "y = df_unificado.iloc[:, -1].values\n",
        "\n",
        "# nome de cada atributo\n",
        "feature_names = df_unificado.columns[:-1]\n",
        "\n",
        "# nome de cada classe\n",
        "target_names = df_unificado.columns[-1]\n",
        "\n",
        "print(f\"Dimens√µes de X: {X.shape}\\n\")\n",
        "print(f\"Dimens√µes de y: {y.shape}\\n\")\n",
        "print(f\"Nomes dos atributos: {feature_names}\\n\")\n",
        "print(f\"Nomes das classes: {target_names}\")"
      ],
      "metadata": {
        "id": "0wQUO3NaD_Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, RepeatedKFold, cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import joblib\n",
        "\n",
        "# 1) Cria um hold-out antes de qualquer CV\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "l_sS3CC5nvLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline([\n",
        "    ('regressor', DecisionTreeRegressor(random_state=42))\n",
        "])\n"
      ],
      "metadata": {
        "id": "vmN6v5YjKpBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RepeatedKFold\n",
        "\n",
        "# 5√ó5 CV repetida: balanceia vi√©s x vari√¢ncia na estima√ß√£o\n",
        "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n"
      ],
      "metadata": {
        "id": "QqqgtLqEIpWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "scoring = {\n",
        "    'R2': 'r2',\n",
        "    'MSE': 'neg_mean_squared_error',\n",
        "    'MAE': 'neg_mean_absolute_error',\n",
        "    'MAPE': 'neg_mean_absolute_percentage_error',\n",
        "    'MedAE': 'neg_median_absolute_error',\n",
        "    'MaxE': 'max_error',\n",
        "    'EVS': 'explained_variance',\n",
        "}\n",
        "\n",
        "cv_results = cross_validate(\n",
        "    pipeline, X_train_full, y_train_full,\n",
        "    cv=cv, scoring=scoring, return_train_score=True, n_jobs=-1\n",
        ")"
      ],
      "metadata": {
        "id": "aBPNGZv-KV3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinar o modelo\n",
        "pipeline.fit(X_train_full, y_train_full)"
      ],
      "metadata": {
        "id": "LWeJ5i_3oKrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Gera predi√ß√µes no hold-out\n",
        "y_pred = pipeline.predict(X_test)"
      ],
      "metadata": {
        "id": "X7ObbwraoPA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Salva TUDO num dict\n",
        "full_results = {\n",
        "    'pipeline': pipeline,\n",
        "    'X_test':   X_test,\n",
        "    'y_test':   y_test,\n",
        "    'y_pred':   y_pred,\n",
        "    'cv_results': cv_results,\n",
        "    'feature_names': feature_names\n",
        "}\n",
        "joblib.dump(full_results, '/content/drive/.../full_results.pkl')"
      ],
      "metadata": {
        "id": "_XbBPvSsoRZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "# Carregar o modelo\n",
        "pipeline = joblib.load('/content/drive/MyDrive/Trabalho ML Mestrado 01-2025/modelo_joblib.pkl')\n"
      ],
      "metadata": {
        "id": "I66pLt1oE-cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib, pandas as pd, streamlit as st, plotly.express as px\n",
        "from sklearn.metrics import (\n",
        "    r2_score, mean_squared_error, mean_absolute_error,\n",
        "    mean_absolute_percentage_error, median_absolute_error,\n",
        "    max_error, explained_variance_score\n",
        ")\n",
        "\n",
        "def report_results(results: dict):\n",
        "    \"\"\"\n",
        "    Gera um relat√≥rio interativo de regress√£o em Streamlit.\n",
        "\n",
        "    Par√¢metros:\n",
        "      results: dict contendo\n",
        "        - 'pipeline': modelo treinado\n",
        "        - 'X_test': array ou DataFrame\n",
        "        - 'y_test': vetor verdadeiro\n",
        "        - opcionalmente 'y_pred', 'feature_names'\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Extrair itens do dict\n",
        "    model = results['pipeline']\n",
        "    X_test = results['X_test']\n",
        "    y_true = results['y_test']\n",
        "    y_pred = results.get('y_pred', model.predict(X_test))\n",
        "    feature_names = results.get('feature_names',\n",
        "                                getattr(X_test, 'columns', None))\n",
        "\n",
        "    # 2) C√°lculo das m√©tricas principais\n",
        "    metrics = {\n",
        "        'R¬≤ Score'           : r2_score(y_true, y_pred),\n",
        "        'MSE'                : mean_squared_error(y_true, y_pred),\n",
        "        'MAE'                : mean_absolute_error(y_true, y_pred),\n",
        "        'MAPE'               : mean_absolute_percentage_error(y_true, y_pred),\n",
        "        'MedAE'              : median_absolute_error(y_true, y_pred),\n",
        "        'Max Error'          : max_error(y_true, y_pred),\n",
        "        'Explained Variance' : explained_variance_score(y_true, y_pred),\n",
        "    }\n",
        "    df_metrics = pd.DataFrame.from_dict(\n",
        "        metrics, orient='index', columns=['Valor']\n",
        "    ).round(4)\n",
        "\n",
        "    # 3) Cabe√ßalho e par√¢metros do modelo\n",
        "    st.title(\"üìä Relat√≥rio de Desempenho do Modelo\")\n",
        "    st.subheader(\"üîß Par√¢metros do Modelo\")\n",
        "    st.json(model.get_params())  # exibe estrutura√ß√£o de hiperpar√¢metros\n",
        "\n",
        "    # 4) Tabela de m√©tricas\n",
        "    st.subheader(\"üìà M√©tricas de Regress√£o\")\n",
        "    st.table(df_metrics)\n",
        "\n",
        "    # 5) Visualiza√ß√£o True vs Predito\n",
        "    st.subheader(\"üîç Dispers√£o Real √ó Predito\")\n",
        "    fig1 = px.scatter(\n",
        "        x=y_true, y=y_pred,\n",
        "        labels={'x':'Real','y':'Predito'},\n",
        "        title=\"Real vs Predito\"\n",
        "    )\n",
        "    # adicionar linha identidade\n",
        "    min_val = min(y_true.min(), y_pred.min())\n",
        "    max_val = max(y_true.max(), y_pred.max())\n",
        "    fig1.add_shape(type=\"line\",\n",
        "                   x0=min_val, y0=min_val,\n",
        "                   x1=max_val, y1=max_val,\n",
        "                   line=dict(dash=\"dash\", color=\"gray\"))\n",
        "    st.plotly_chart(fig1, use_container_width=True)\n",
        "\n",
        "    # 6) Distribui√ß√£o de res√≠duos\n",
        "    st.subheader(\"üìâ Histograma de Res√≠duos\")\n",
        "    residuals = y_true - y_pred\n",
        "    fig2 = px.histogram(\n",
        "        residuals, nbins=50,\n",
        "        labels={'value': 'Res√≠duo (Real ‚Äì Predito)'},\n",
        "        title=\"Distribui√ß√£o de Res√≠duos\"\n",
        "    )\n",
        "    st.plotly_chart(fig2, use_container_width=True)\n",
        "\n",
        "    # 7) Boxplot de res√≠duos por quartil (opcional)\n",
        "    st.subheader(\"üóÇÔ∏è Res√≠duos por Quartil de Real\")\n",
        "    df_r = pd.DataFrame({'Real': y_true, 'Res√≠duo': residuals})\n",
        "    df_r['Quartil'] = pd.qcut(df_r['Real'], 4, labels=False)\n",
        "    fig3 = px.box(\n",
        "        df_r, x='Quartil', y='Res√≠duo',\n",
        "        title=\"Boxplot de Res√≠duos por Quartil de Valor Real\"\n",
        "    )\n",
        "    st.plotly_chart(fig3, use_container_width=True)\n",
        "\n",
        "    # 8) Import√¢ncia das features\n",
        "    if hasattr(model, 'feature_importances_') and feature_names is not None:\n",
        "        st.subheader(\"‚≠ê Import√¢ncia das Features\")\n",
        "        fi = pd.DataFrame({\n",
        "            'feature': feature_names,\n",
        "            'importance': model.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "        st.bar_chart(fi.set_index('feature'))\n",
        "\n",
        "    # 9) Download dos resultados\n",
        "    st.subheader(\"üì• Exportar Relat√≥rio\")\n",
        "    csv_metrics = df_metrics.to_csv().encode('utf-8')\n",
        "    st.download_button(\n",
        "        label=\"Baixar M√©tricas (CSV)\",\n",
        "        data=csv_metrics,\n",
        "        file_name='metrics_report.csv',\n",
        "        mime='text/csv'\n",
        "    )\n"
      ],
      "metadata": {
        "id": "KJzfeCmXFTJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vari√¢ncia nas √Årvores de Decis√£o- EDITAR TUDO PARA REGRE√á√ÉO\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "60j9N1m-gnf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analisando a Estrutura das √Årvores\n",
        "\n",
        "Como estudado em aula, a √°rvore de decis√£o √© conhecida por ser um classificador com alta vari√¢ncia. Isso possui consequ√™ncias na estrutura das √°rvores treinadas.\n",
        "\n",
        "O c√≥digo abaixo treina v√°rias √°rvores de decis√£o com diferentes conjuntos de treino obtidos atrav√©s do m√©todo holdout.\n",
        "Use-o para responder √† Quest√£o 1 do question√°rio.\n"
      ],
      "metadata": {
        "id": "iMAD6e-vT5I6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### An√°lise da Varia√ß√£o na Acur√°cia\n",
        "\n",
        "A propriedade de vari√¢ncia tamb√©m implica em efeitos na variabilidade do desempenho dos modelos. Para fins de exemplo, podemos usar a acur√°cia como medida de desempenho atrav√©s das fun√ß√µes do scikit-learn. Entretanto, outras m√©tricas de desempenho como Recall e Precis√£o, que s√£o mais indicadas para problemas em que o n√∫mero de inst√¢ncias por classe √© desbalanceado (como √© o caso deste conjunto de dados) poderiam tamb√©m ser exploradas (a crit√©rio do aluno, podem ser adicionadas para observa√ß√£o, mas a quest√£o deve ser respondida com base na acur√°cia)."
      ],
      "metadata": {
        "id": "4dE3IWkdlpVP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O c√≥digo abaixo executa repetidas vezes o treinamento das √°rvores de decis√£o, da mesma forma que no item *Analisando a Estrutura das √Årvores*.\n",
        "Modifique-o de forma a obter a acur√°cia para cada execu√ß√£o e ent√£o calcule a m√©dia, desvio padr√£o, m√°ximo e m√≠nimo dos valores. Use esses resultados para responder √† **Quest√£o 2**.\n",
        "\n",
        "**Aten√ß√£o: N√£o mude os valores que est√£o sendo passados para os par√¢metros random_state para garantir a reprodutibilidade do c√≥digo**.\n"
      ],
      "metadata": {
        "id": "Dp5K0jyaLduN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### An√°lise de Inst√¢ncia individuais\n",
        "\n",
        "1. Treine novamente uma √°rvore de decis√£o usando um novo conjunto de treino gerado com a fun√ß√£o train_test_split. Utilize 20% de dados de teste e, desta vez, n√£o **especifique valor nenhum para o random_state**.\n",
        "\n",
        "2. Fa√ßa a predi√ß√£o para as inst√¢ncias especificadas abaixo e preencha na tabela do excel indicada no **Moodle** a classifica√ß√£o encontrada (0 para maligno e 1 para benigno).\n"
      ],
      "metadata": {
        "id": "OrsF5WMepURZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## O Efeito da Poda"
      ],
      "metadata": {
        "id": "AZelTK5blG_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As √°rvores de decis√£o treinadas nos itens anteriores n√£o possu√≠am nenhuma forma de poda. No entanto, √© poss√≠vel utilizar t√©cnicas de poda atrav√©s do scikit-learn. Como consequ√™ncia, elas podem ter uma complexidade al√©m do que √© necess√°rio na modelagem do problema.\n",
        "\n"
      ],
      "metadata": {
        "id": "IYchPiY3lPMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo de Pr√©-poda: profundidade m√°xima da √°rvore\n",
        "Podemos especificar a profundidade m√°xima da √°rvore utilizando o par√¢metro max_depth."
      ],
      "metadata": {
        "id": "rKvCQYSjovEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O c√≥digo abaixo gera √°rvores de decis√£o com diferentes profundidades m√°ximas e as avalia em termos de acur√°cia.\n",
        "\n",
        "Observe que todas as √°rvores s√£o treinadas e avaliadas com os mesmos conjuntos de treino e teste, visto que especificamos o par√¢metro $random\\_state = 0$.\n",
        "\n",
        "Com base nesse c√≥digo, e poss√≠veis modifica√ß√µes que voc√™ fa√ßa a ele, responda √† **Quest√£o  4** do question√°rio.\n",
        "\n",
        "**N√£o mude o valor que est√° sendo passado em random_state=0**.\n"
      ],
      "metadata": {
        "id": "5bzmcFPutJR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo de P√≥s-poda: Custo-complexidade\n",
        "\n",
        "A biblioteca scikit-learn possui uma implementa√ß√£o de p√≥s-poda por custo-complexidade, baseada no par√¢metro de custo-complexidade $\\alpha \\ge 0$.\n",
        "\n",
        "Na implementa√ß√£o descrita na biblioteca, √© definido tamb√©m um custo-complexidade efetivo do nodo. Quanto maior for a taxa de erros ao se podar a sub√°rvore de um nodo, maior ser√° seu custo-complexidade efetivo. Al√©m disso, quanto maior for a complexidade (n√∫mero de nodos terminais) da sub√°rvore do nodo, menor ser√° seu custo-complexidade efetivo.\n",
        "Em resumo, um nodo com alto custo-complexidade efetivo √© um nodo importante para diminuir a taxa de erros e com baixa complexidade.\n",
        "\n",
        "Dentro da biblioteca, passamos um par√¢metro $ccp\\_alpha$ que serve como um custo-complexidade efetivo de corte: sub√°rvores s√£o podadas enquanto houver nodos com custo-complexidade menor do que o par√¢metro $ccp\\_alpha$.\n",
        "Ou seja, quando maior for o par√¢metro, mais intensa ser√° a poda.\n",
        "\n",
        "Para mais informa√ß√µes:\n",
        "* https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning\n",
        "* https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html\n",
        "\n",
        "Use o c√≥digo abaixo para resolver √† **Quest√£o 5**."
      ],
      "metadata": {
        "id": "3IHz5Y-KvrCI"
      }
    }
  ]
}