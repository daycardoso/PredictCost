{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daycardoso/PredictCost/blob/main/PredictCostRandonForest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trabalho CMP263 - Aprendizagem de Máquina - INF/UFRGS\n",
        "\n",
        "## Modelo 1 - Decision tree sem poda\n",
        "\n",
        "As árvores de decisão são conhecidas por possuírem um baixo viés, ao mesmo tempo em que apresentam alta variância.\n",
        "Isto é, o método é capaz de modelar fronteiras de decisão bastante complexas, o que, por um lado, é positivo, mas torna o algoritmo bastante suscetível a ruído ou a padrões nos dados de treino que não generalizam para instâncias de teste.\n",
        "Por isso, técnicas de poda são fundamentais para o uso efetivo do modelo em dados novos.\n",
        "\n",
        "Nessa atividade, iremos analisar como a estrutura e as predições da árvore de decisão são afetadas por pequenas variações no conjunto de treino. Além disso, veremos duas técnicas de poda que podem ser usadas para controlar a complexidade do modelo.\n",
        "\n",
        "**Este *colab* deve ser usado como base para o preenchimento do questionário encontrado no Moodle. Faça uma cópia do mesmo para realizar o exercício.** A forma mais fácil para duplicar este *colab* é ir em File > \"Save a Copy in Drive\". Não é necessário entregar este *colab* preenchido, mas guarde-o para caso ache que algum questionário está errado.\n"
      ],
      "metadata": {
        "id": "E650rvooE65t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objetivos da Atividade\n",
        "* Analisar os impactos da característica de **variância** nas árvores de decisão.\n",
        "* Analisar o efeito da **poda** em árvores de decisão.\n"
      ],
      "metadata": {
        "id": "bzW8jdj7Jgp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregamento dos Dados\n"
      ],
      "metadata": {
        "id": "owi-J_whK4IS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Obtenção e análise dos dados\n",
        "O código abaixo carrega o dataset do kaggle e mostra algumas informações básicas sobre os dados"
      ],
      "metadata": {
        "id": "SqJsx9OITVn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gqs1uPW95__c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# import glob\n",
        "\n",
        "# arquivos = glob.glob('/content/drive/MyDrive/Trabalho ML Mestrado 01-2025/*.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "rfTSbcxF_Ei6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dfs = [pd.read_csv(f) for f in arquivos]\n",
        "# df_unificado = pd.concat(dfs, axis=0, ignore_index=True)\n",
        "# df_unificado.head()"
      ],
      "metadata": {
        "id": "ACuMo2cjIP7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Excluir a coluna type\n",
        "# df_unificado = df_unificado.drop('type', axis=1)\n",
        "# df_unificado.head()"
      ],
      "metadata": {
        "id": "TyjiqnDQDhTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Garantir que não a duplicata de instancias evitando sobreposição entre os dados de treinamento e teste\n",
        "# df_unificado = df_unificado.drop_duplicates().reset_index(drop=True)\n",
        "# df_unificado.head(-50)"
      ],
      "metadata": {
        "id": "w6WTOZinHWWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvar o dataset completo, sem duplicatas\n",
        "# df_unificado.to_csv('/content/drive/MyDrive/Trabalho ML Mestrado 01-2025/df_unificado.csv', index=False)"
      ],
      "metadata": {
        "id": "P5TiN0ACFnPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o datset unificado\n",
        "df_unificado = pd.read_csv('/content/drive/MyDrive/Trabalho ML Mestrado 01-2025/df_unificado.csv')"
      ],
      "metadata": {
        "id": "zGi6kt1-Hiai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# matriz contendo os atributos\n",
        "X = df_unificado.iloc[:, :-1].values\n",
        "\n",
        "# vetor contendo o custo, ou seja, a ultima coluna\n",
        "y = df_unificado.iloc[:, -1].values\n",
        "\n",
        "# nome de cada atributo\n",
        "feature_names = df_unificado.columns[:-1]\n",
        "\n",
        "# nome de cada classe\n",
        "target_names = df_unificado.columns[-1]\n",
        "\n",
        "print(f\"Dimensões de X: {X.shape}\\n\")\n",
        "print(f\"Dimensões de y: {y.shape}\\n\")\n",
        "print(f\"Nomes dos atributos: {feature_names}\\n\")\n",
        "print(f\"Nomes das classes: {target_names}\")"
      ],
      "metadata": {
        "id": "0wQUO3NaD_Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, RepeatedKFold, cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import joblib\n",
        "\n",
        "# 1) Cria um hold-out antes de qualquer CV\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "l_sS3CC5nvLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline([\n",
        "    ('regressor', DecisionTreeRegressor(random_state=42))\n",
        "])\n"
      ],
      "metadata": {
        "id": "vmN6v5YjKpBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RepeatedKFold\n",
        "\n",
        "# 5×5 CV repetida: balanceia viés x variância na estimação\n",
        "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n"
      ],
      "metadata": {
        "id": "QqqgtLqEIpWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "scoring = {\n",
        "    'R2': 'r2',\n",
        "    'MSE': 'neg_mean_squared_error',\n",
        "    'MAE': 'neg_mean_absolute_error',\n",
        "    'MAPE': 'neg_mean_absolute_percentage_error',\n",
        "    'MedAE': 'neg_median_absolute_error',\n",
        "    'MaxE': 'max_error',\n",
        "    'EVS': 'explained_variance',\n",
        "}\n",
        "\n",
        "cv_results = cross_validate(\n",
        "    pipeline, X_train_full, y_train_full,\n",
        "    cv=cv, scoring=scoring, return_train_score=True, n_jobs=-1\n",
        ")"
      ],
      "metadata": {
        "id": "aBPNGZv-KV3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinar o modelo\n",
        "pipeline.fit(X_train_full, y_train_full)"
      ],
      "metadata": {
        "id": "LWeJ5i_3oKrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Gera predições no hold-out\n",
        "y_pred = pipeline.predict(X_test)"
      ],
      "metadata": {
        "id": "X7ObbwraoPA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Salva TUDO num dict\n",
        "full_results = {\n",
        "    'pipeline': pipeline,\n",
        "    'X_test':   X_test,\n",
        "    'y_test':   y_test,\n",
        "    'y_pred':   y_pred,\n",
        "    'cv_results': cv_results,\n",
        "    'feature_names': feature_names\n",
        "}\n",
        "joblib.dump(full_results, '/content/drive/.../full_results.pkl')"
      ],
      "metadata": {
        "id": "_XbBPvSsoRZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "# Carregar o modelo\n",
        "pipeline = joblib.load('/content/drive/MyDrive/Trabalho ML Mestrado 01-2025/modelo_joblib.pkl')\n"
      ],
      "metadata": {
        "id": "I66pLt1oE-cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib, pandas as pd, streamlit as st, plotly.express as px\n",
        "from sklearn.metrics import (\n",
        "    r2_score, mean_squared_error, mean_absolute_error,\n",
        "    mean_absolute_percentage_error, median_absolute_error,\n",
        "    max_error, explained_variance_score\n",
        ")\n",
        "\n",
        "def report_results(results: dict):\n",
        "    \"\"\"\n",
        "    Gera um relatório interativo de regressão em Streamlit.\n",
        "\n",
        "    Parâmetros:\n",
        "      results: dict contendo\n",
        "        - 'pipeline': modelo treinado\n",
        "        - 'X_test': array ou DataFrame\n",
        "        - 'y_test': vetor verdadeiro\n",
        "        - opcionalmente 'y_pred', 'feature_names'\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Extrair itens do dict\n",
        "    model = results['pipeline']\n",
        "    X_test = results['X_test']\n",
        "    y_true = results['y_test']\n",
        "    y_pred = results.get('y_pred', model.predict(X_test))\n",
        "    feature_names = results.get('feature_names',\n",
        "                                getattr(X_test, 'columns', None))\n",
        "\n",
        "    # 2) Cálculo das métricas principais\n",
        "    metrics = {\n",
        "        'R² Score'           : r2_score(y_true, y_pred),\n",
        "        'MSE'                : mean_squared_error(y_true, y_pred),\n",
        "        'MAE'                : mean_absolute_error(y_true, y_pred),\n",
        "        'MAPE'               : mean_absolute_percentage_error(y_true, y_pred),\n",
        "        'MedAE'              : median_absolute_error(y_true, y_pred),\n",
        "        'Max Error'          : max_error(y_true, y_pred),\n",
        "        'Explained Variance' : explained_variance_score(y_true, y_pred),\n",
        "    }\n",
        "    df_metrics = pd.DataFrame.from_dict(\n",
        "        metrics, orient='index', columns=['Valor']\n",
        "    ).round(4)\n",
        "\n",
        "    # 3) Cabeçalho e parâmetros do modelo\n",
        "    st.title(\"📊 Relatório de Desempenho do Modelo\")\n",
        "    st.subheader(\"🔧 Parâmetros do Modelo\")\n",
        "    st.json(model.get_params())  # exibe estruturação de hiperparâmetros\n",
        "\n",
        "    # 4) Tabela de métricas\n",
        "    st.subheader(\"📈 Métricas de Regressão\")\n",
        "    st.table(df_metrics)\n",
        "\n",
        "    # 5) Visualização True vs Predito\n",
        "    st.subheader(\"🔍 Dispersão Real × Predito\")\n",
        "    fig1 = px.scatter(\n",
        "        x=y_true, y=y_pred,\n",
        "        labels={'x':'Real','y':'Predito'},\n",
        "        title=\"Real vs Predito\"\n",
        "    )\n",
        "    # adicionar linha identidade\n",
        "    min_val = min(y_true.min(), y_pred.min())\n",
        "    max_val = max(y_true.max(), y_pred.max())\n",
        "    fig1.add_shape(type=\"line\",\n",
        "                   x0=min_val, y0=min_val,\n",
        "                   x1=max_val, y1=max_val,\n",
        "                   line=dict(dash=\"dash\", color=\"gray\"))\n",
        "    st.plotly_chart(fig1, use_container_width=True)\n",
        "\n",
        "    # 6) Distribuição de resíduos\n",
        "    st.subheader(\"📉 Histograma de Resíduos\")\n",
        "    residuals = y_true - y_pred\n",
        "    fig2 = px.histogram(\n",
        "        residuals, nbins=50,\n",
        "        labels={'value': 'Resíduo (Real – Predito)'},\n",
        "        title=\"Distribuição de Resíduos\"\n",
        "    )\n",
        "    st.plotly_chart(fig2, use_container_width=True)\n",
        "\n",
        "    # 7) Boxplot de resíduos por quartil (opcional)\n",
        "    st.subheader(\"🗂️ Resíduos por Quartil de Real\")\n",
        "    df_r = pd.DataFrame({'Real': y_true, 'Resíduo': residuals})\n",
        "    df_r['Quartil'] = pd.qcut(df_r['Real'], 4, labels=False)\n",
        "    fig3 = px.box(\n",
        "        df_r, x='Quartil', y='Resíduo',\n",
        "        title=\"Boxplot de Resíduos por Quartil de Valor Real\"\n",
        "    )\n",
        "    st.plotly_chart(fig3, use_container_width=True)\n",
        "\n",
        "    # 8) Importância das features\n",
        "    if hasattr(model, 'feature_importances_') and feature_names is not None:\n",
        "        st.subheader(\"⭐ Importância das Features\")\n",
        "        fi = pd.DataFrame({\n",
        "            'feature': feature_names,\n",
        "            'importance': model.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "        st.bar_chart(fi.set_index('feature'))\n",
        "\n",
        "    # 9) Download dos resultados\n",
        "    st.subheader(\"📥 Exportar Relatório\")\n",
        "    csv_metrics = df_metrics.to_csv().encode('utf-8')\n",
        "    st.download_button(\n",
        "        label=\"Baixar Métricas (CSV)\",\n",
        "        data=csv_metrics,\n",
        "        file_name='metrics_report.csv',\n",
        "        mime='text/csv'\n",
        "    )\n"
      ],
      "metadata": {
        "id": "KJzfeCmXFTJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variância nas Árvores de Decisão- EDITAR TUDO PARA REGREÇÃO\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "60j9N1m-gnf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analisando a Estrutura das Árvores\n",
        "\n",
        "Como estudado em aula, a árvore de decisão é conhecida por ser um classificador com alta variância. Isso possui consequências na estrutura das árvores treinadas.\n",
        "\n",
        "O código abaixo treina várias árvores de decisão com diferentes conjuntos de treino obtidos através do método holdout.\n",
        "Use-o para responder à Questão 1 do questionário.\n"
      ],
      "metadata": {
        "id": "iMAD6e-vT5I6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Análise da Variação na Acurácia\n",
        "\n",
        "A propriedade de variância também implica em efeitos na variabilidade do desempenho dos modelos. Para fins de exemplo, podemos usar a acurácia como medida de desempenho através das funções do scikit-learn. Entretanto, outras métricas de desempenho como Recall e Precisão, que são mais indicadas para problemas em que o número de instâncias por classe é desbalanceado (como é o caso deste conjunto de dados) poderiam também ser exploradas (a critério do aluno, podem ser adicionadas para observação, mas a questão deve ser respondida com base na acurácia)."
      ],
      "metadata": {
        "id": "4dE3IWkdlpVP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo executa repetidas vezes o treinamento das árvores de decisão, da mesma forma que no item *Analisando a Estrutura das Árvores*.\n",
        "Modifique-o de forma a obter a acurácia para cada execução e então calcule a média, desvio padrão, máximo e mínimo dos valores. Use esses resultados para responder à **Questão 2**.\n",
        "\n",
        "**Atenção: Não mude os valores que estão sendo passados para os parâmetros random_state para garantir a reprodutibilidade do código**.\n"
      ],
      "metadata": {
        "id": "Dp5K0jyaLduN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Análise de Instância individuais\n",
        "\n",
        "1. Treine novamente uma árvore de decisão usando um novo conjunto de treino gerado com a função train_test_split. Utilize 20% de dados de teste e, desta vez, não **especifique valor nenhum para o random_state**.\n",
        "\n",
        "2. Faça a predição para as instâncias especificadas abaixo e preencha na tabela do excel indicada no **Moodle** a classificação encontrada (0 para maligno e 1 para benigno).\n"
      ],
      "metadata": {
        "id": "OrsF5WMepURZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## O Efeito da Poda"
      ],
      "metadata": {
        "id": "AZelTK5blG_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As árvores de decisão treinadas nos itens anteriores não possuíam nenhuma forma de poda. No entanto, é possível utilizar técnicas de poda através do scikit-learn. Como consequência, elas podem ter uma complexidade além do que é necessário na modelagem do problema.\n",
        "\n"
      ],
      "metadata": {
        "id": "IYchPiY3lPMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo de Pré-poda: profundidade máxima da árvore\n",
        "Podemos especificar a profundidade máxima da árvore utilizando o parâmetro max_depth."
      ],
      "metadata": {
        "id": "rKvCQYSjovEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo gera árvores de decisão com diferentes profundidades máximas e as avalia em termos de acurácia.\n",
        "\n",
        "Observe que todas as árvores são treinadas e avaliadas com os mesmos conjuntos de treino e teste, visto que especificamos o parâmetro $random\\_state = 0$.\n",
        "\n",
        "Com base nesse código, e possíveis modificações que você faça a ele, responda à **Questão  4** do questionário.\n",
        "\n",
        "**Não mude o valor que está sendo passado em random_state=0**.\n"
      ],
      "metadata": {
        "id": "5bzmcFPutJR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo de Pós-poda: Custo-complexidade\n",
        "\n",
        "A biblioteca scikit-learn possui uma implementação de pós-poda por custo-complexidade, baseada no parâmetro de custo-complexidade $\\alpha \\ge 0$.\n",
        "\n",
        "Na implementação descrita na biblioteca, é definido também um custo-complexidade efetivo do nodo. Quanto maior for a taxa de erros ao se podar a subárvore de um nodo, maior será seu custo-complexidade efetivo. Além disso, quanto maior for a complexidade (número de nodos terminais) da subárvore do nodo, menor será seu custo-complexidade efetivo.\n",
        "Em resumo, um nodo com alto custo-complexidade efetivo é um nodo importante para diminuir a taxa de erros e com baixa complexidade.\n",
        "\n",
        "Dentro da biblioteca, passamos um parâmetro $ccp\\_alpha$ que serve como um custo-complexidade efetivo de corte: subárvores são podadas enquanto houver nodos com custo-complexidade menor do que o parâmetro $ccp\\_alpha$.\n",
        "Ou seja, quando maior for o parâmetro, mais intensa será a poda.\n",
        "\n",
        "Para mais informações:\n",
        "* https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning\n",
        "* https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html\n",
        "\n",
        "Use o código abaixo para resolver à **Questão 5**."
      ],
      "metadata": {
        "id": "3IHz5Y-KvrCI"
      }
    }
  ]
}